

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Archive Layout with Content - Vitor Guizilini</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Vitor Guizilini">
<meta property="og:title" content="Archive Layout with Content">


  <link rel="canonical" href="http://localhost:4000/archive-layout-with-content/">
  <meta property="og:url" content="http://localhost:4000/archive-layout-with-content/">







  

  












  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Vitor Guizilini",
      "url" : "http://localhost:4000",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Vitor Guizilini Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="http://localhost:4000/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="http://localhost:4000/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="http://localhost:4000/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="http://localhost:4000/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="http://localhost:4000/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="http://localhost:4000/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="http://localhost:4000/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:4000/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="http://localhost:4000/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="http://localhost:4000/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="http://localhost:4000/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="http://localhost:4000/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="http://localhost:4000/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.css"/>


<!-- Support for MatJax -->
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="http://localhost:4000/">Vitor Guizilini</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/publications/">Publications</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    



<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="http://localhost:4000/images/vitor_profile.jpg" class="author__avatar" alt="Vitor Guizilini <br> Staff Research Scientist">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Vitor Guizilini <br> Staff Research Scientist</h3>
    
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      <!-- Font Awesome icons / Biographic information  -->
      
      
        <br><li><a href="https://tri.global"><i class="fa fa-solid fa-building-columns icon-pad-right" aria-hidden="true"></i>Toyota Research Institute (TRI)</a></li>
      
      
      
        <li><a href="mailto:vitor.guizilini@tri.global"><i class="fas fa-fw fa-envelope icon-pad-right" aria-hidden="true"></i>Email Address</a></li>
      

      <!-- Font Awesome and Academicons icons / Academic websites -->
            
      
        <li><a href="https://scholar.google.com.br/citations?user=ow3r9ogAAAAJ&hl=en"><i class="ai ai-google-scholar icon-pad-right"></i>Google Scholar</a></li>
      
      
      
        <li><a href="https://orcid.org/0000-0002-8715-8307"><i class="ai ai-orcid ai-fw icon-pad-right"></i>ORCID</a></li>
      
                              
      

      <!-- Font Awesome icons / Repositories and software development -->
      
            
            
      
        <li><a href="https://github.com/https://github.com/tri-ml/vidar"><i class="fab fa-fw fa-github icon-pad-right" aria-hidden="true"></i>Github</a></li>
      
            
            

      <!-- Font Awesome icons / Social media -->
      
      
            
      
                  
                  
      
            
            
      
        <li><a href="https://www.linkedin.com/in/https://www.linkedin.com/in/vitorguizilini"><i class="fab fa-fw fa-linkedin icon-pad-right" aria-hidden="true"></i>LinkedIn</a></li>
            
      
            
                  
            
      
            
            
      
              
      
                      
      
      
            
    </ul>
  </div>
</div>

  
  </div>


  <div class="archive">
    
      <h1 class="page__title">Archive Layout with Content</h1>
    
    <p>A variety of common markup showing how the theme styles them.</p>

<h1 id="header-one">Header one</h1>

<h2 id="header-two">Header two</h2>

<h3 id="header-three">Header three</h3>

<h4 id="header-four">Header four</h4>

<h5 id="header-five">Header five</h5>

<h6 id="header-six">Header six</h6>

<h2 id="blockquotes">Blockquotes</h2>

<p>Single line blockquote:</p>

<blockquote>
  <p>Quotes are cool.</p>
</blockquote>

<h2 id="tables">Tables</h2>

<table>
  <thead>
    <tr>
      <th>Entry</th>
      <th>Item</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="#">John Doe</a></td>
      <td>2016</td>
      <td>Description of the item in the list</td>
    </tr>
    <tr>
      <td><a href="#">Jane Doe</a></td>
      <td>2019</td>
      <td>Description of the item in the list</td>
    </tr>
    <tr>
      <td><a href="#">Doe Doe</a></td>
      <td>2022</td>
      <td>Description of the item in the list</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Header1</th>
      <th style="text-align: center">Header2</th>
      <th style="text-align: right">Header3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">cell1</td>
      <td style="text-align: center">cell2</td>
      <td style="text-align: right">cell3</td>
    </tr>
    <tr>
      <td style="text-align: left">cell4</td>
      <td style="text-align: center">cell5</td>
      <td style="text-align: right">cell6</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td style="text-align: left">cell1</td>
      <td style="text-align: center">cell2</td>
      <td style="text-align: right">cell3</td>
    </tr>
    <tr>
      <td style="text-align: left">cell4</td>
      <td style="text-align: center">cell5</td>
      <td style="text-align: right">cell6</td>
    </tr>
  </tbody>
  <tfoot>
    <tr>
      <td style="text-align: left">Foot1</td>
      <td style="text-align: center">Foot2</td>
      <td style="text-align: right">Foot3</td>
    </tr>
  </tfoot>
</table>

<h2 id="definition-lists">Definition Lists</h2>

<dl>
  <dt>Definition List Title</dt>
  <dd>Definition list division.</dd>
  <dt>Startup</dt>
  <dd>A startup company or startup is a company or temporary organization designed to search for a repeatable and scalable business model.</dd>
  <dt>#dowork</dt>
  <dd>Coined by Rob Dyrdek and his personal body guard Christopher “Big Black” Boykins, “Do Work” works as a self motivator, to motivating your friends.</dd>
  <dt>Do It Live</dt>
  <dd>I’ll let Bill O’Reilly <a href="https://www.youtube.com/watch?v=O_HyZ5aW76c" title="We'll Do It Live">explain</a> this one.</dd>
</dl>

<h2 id="unordered-lists-nested">Unordered Lists (Nested)</h2>

<ul>
  <li>List item one
    <ul>
      <li>List item one
        <ul>
          <li>List item one</li>
          <li>List item two</li>
          <li>List item three</li>
          <li>List item four</li>
        </ul>
      </li>
      <li>List item two</li>
      <li>List item three</li>
      <li>List item four</li>
    </ul>
  </li>
  <li>List item two</li>
  <li>List item three</li>
  <li>List item four</li>
</ul>

<h2 id="ordered-list-nested">Ordered List (Nested)</h2>

<ol>
  <li>List item one
    <ol>
      <li>List item one
        <ol>
          <li>List item one</li>
          <li>List item two</li>
          <li>List item three</li>
          <li>List item four</li>
        </ol>
      </li>
      <li>List item two</li>
      <li>List item three</li>
      <li>List item four</li>
    </ol>
  </li>
  <li>List item two</li>
  <li>List item three</li>
  <li>List item four</li>
</ol>

<h2 id="buttons">Buttons</h2>

<p>Make any link standout more when applying the <code class="language-plaintext highlighter-rouge">.btn</code> class.</p>

<h2 id="notices">Notices</h2>

<p class="notice"><strong>Watch out!</strong> You can also add notices by appending <code class="language-plaintext highlighter-rouge">{: .notice}</code> to a paragraph.</p>

<h2 id="html-tags">HTML Tags</h2>

<h3 id="address-tag">Address Tag</h3>

<address>
  1 Infinite Loop<br /> Cupertino, CA 95014<br /> United States
</address>

<h3 id="anchor-tag-aka-link">Anchor Tag (aka. Link)</h3>

<p>This is an example of a <a href="http://github.com" title="Github">link</a>.</p>

<h3 id="abbreviation-tag">Abbreviation Tag</h3>

<p>The abbreviation <abbr title="Cascading Style Sheets">CSS</abbr> stands for “Cascading Style Sheets”.</p>

<h3 id="cite-tag">Cite Tag</h3>

<p>“Code is poetry.” —<cite>Automattic</cite></p>

<h3 id="code-tag">Code Tag</h3>

<p>You will learn later on in these tests that <code class="language-plaintext highlighter-rouge">word-wrap: break-word;</code> will be your best friend.</p>

<h3 id="strike-tag">Strike Tag</h3>

<p>This tag will let you <strike>strikeout text</strike>.</p>

<h3 id="emphasize-tag">Emphasize Tag</h3>

<p>The emphasize tag should <em>italicize</em> text.</p>

<h3 id="insert-tag">Insert Tag</h3>

<p>This tag should denote <ins>inserted</ins> text.</p>

<h3 id="keyboard-tag">Keyboard Tag</h3>

<p>This scarcely known tag emulates <kbd>keyboard text</kbd>, which is usually styled like the <code class="language-plaintext highlighter-rouge">&lt;code&gt;</code> tag.</p>

<h3 id="preformatted-tag">Preformatted Tag</h3>

<p>This tag styles large blocks of code.</p>

<pre>
.post-title {
  margin: 0 0 5px;
  font-weight: bold;
  font-size: 38px;
  line-height: 1.2;
  and here's a line of some really, really, really, really long text, just to see how the PRE tag handles it and to find out how it overflows;
}
</pre>

<h3 id="quote-tag">Quote Tag</h3>

<p><q>Developers, developers, developers…</q> –Steve Ballmer</p>

<h3 id="strong-tag">Strong Tag</h3>

<p>This tag shows <strong>bold text</strong>.</p>

<h3 id="subscript-tag">Subscript Tag</h3>

<p>Getting our science styling on with H<sub>2</sub>O, which should push the “2” down.</p>

<h3 id="superscript-tag">Superscript Tag</h3>

<p>Still sticking with science and Isaac Newton’s E = MC<sup>2</sup>, which should lift the 2 up.</p>

<h3 id="variable-tag">Variable Tag</h3>

<p>This allows you to denote <var>variables</var>.</p>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title"></h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title"></h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"> <a href="https://arxiv.org/abs/2404.00815"> <img style="margin-right:10px" align="right" width="40" src="/images/arxiv.png" /></a> <a href="https://github.com/hancyran/LiDAR-Diffusion"> <img style="margin-right:10px" align="right" width="40" src="/images/github.png" /></a> <a href="https://www.youtube.com/watch?v=Vj7DubNZnDo"> <img style="margin-right:10px" align="right" width="40" src="/images/youtube.png" /></a> <a href="http://www.google4.com"> <img style="margin-right:10px" align="right" width="40" src="/images/music.png" /></a> </a>
    <h2 class="archive__item-title"> <a href="https://sites.google.com/view/tri-sesc" style="text-decoration:none;color:#000000;text-align:justify;"> Robust Self-Supervised Extrinsic Self-Calibration </a> </h2>
    
    

        
          <p><i> <p> Takayuki Kanai, Igor Vasiljevic, Vitor Guizilini, Adrien Gaidon, Rares Ambrus <br /> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023) </p> </i>  </p>
        

    <p align="justify"><a style="text-decoration:none; color:#000000; align:justify;"> Autonomous vehicles and robots need to operate over a wide variety of scenarios in order to complete tasks efficiently and safely. Multi-camera self-supervised monocular depth estimation from videos is a promising way to reason about the environment, as it generates metrically scaled geometric predictions from visual data without requiring additional sensors. However, most works assume well-calibrated extrinsics to fully leverage this multi-camera setup, even though accurate and efficient calibration is still a challenging problem. In this work, we introduce a novel method for extrinsic calibration that builds upon the principles of self-supervised monocular depth and ego-motion learning. Our proposed curriculum learning strategy uses monocular depth and pose estimators with velocity supervision to estimate extrinsics, and then jointly learns extrinsic calibration along with depth and pose for a set of overlapping cameras rigidly attached to a moving vehicle. Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our method enables self-calibration in various scenes robustly and efficiently compared to a traditional vision-based pose estimation pipeline. Furthermore, we demonstrate the benefits of extrinsics self-calibration as a way to improve depth prediction via joint optimization. </a></p>
<p><img src="/images/map2lidar.gif" align="right" width="100%" margin-bottom="50px" /></p>

    
    
      <br /><p align="justify"><tt><br /> @inproceedings{tri_sesc_iros23, author = {Takayuki Kanai and Igor Vasiljevic and Vitor Guizilini and Adrien Gaidon and Rares Ambrus}, title = {Robust Self-Supervised Extrinsic Self-Calibration}, booktitle = {The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, year = {2023} } </tt></p>
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title">Page Not Found</h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title"></h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title">Archive Layout with Content</h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title">Posts by Category</h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title">Posts by Collection</h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title">CV</h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title"></h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title">Markdown</h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title">Page not in menu</h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title">Page Archive</h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title">News</h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title">Publications</h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title">Publications</h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title">Sitemap</h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title">Posts by Tags</h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title">Talk map</h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title">Talks and presentations</h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title">Teaching</h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title">Terms and Privacy Policy</h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title">Blog posts</h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title"></h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title"></h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title"></h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title"></h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title"></h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title"></h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title"></h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title"></h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title"></h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title"></h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title">Jupyter notebook markdown generator</h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title"></h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title"></h2>
    
    

        

    

    
    









<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <a style="text-decoration:none;color:#000000;text-align:justify;"></a>
    <h2 class="archive__item-title"></h2>
    
    

        

    

    
    


</article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div></article></div>

  </div>
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        


<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
      <li><a href="http://github.com/https://github.com/tri-ml/vidar"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    
    <li><a href="http://localhost:4000/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>


<div class="page__footer-copyright">&copy; 2024 Vitor Guizilini. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="http://localhost:4000/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>






  </body>
</html>

